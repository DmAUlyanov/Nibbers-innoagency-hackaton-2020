{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains code for training and feature vectors calculating using convolutional autoencoder. Vectors produced by recurrent ae show better results, so we didn't use this vectors in submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, dataloader\n",
    "from statistics import mean\n",
    "import visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.filenames = [s.split('.')[0] for s in pd.read_csv('./metrics.csv')['Case'].tolist()]\n",
    "        self.images = []\n",
    "        \n",
    "        for filename in self.filenames:\n",
    "            origin_img = np.array(Image.open(f'./Dataset/Origin/{filename}.png').resize((16, 16)))\n",
    "            if len(origin_img.shape) == 3:\n",
    "                origin_img = origin_img[:, :, 0]\n",
    "            self.images.append(\n",
    "                (\n",
    "                    np.stack([origin_img,\n",
    "                              np.array(Image.open(f'./Dataset/Expert/{filename}_expert.png').resize((16, 16)))]) / 255,\n",
    "                    np.stack([origin_img,\n",
    "                              np.array(Image.open(f'./Dataset/sample_1/{filename}_s1.png').resize((16, 16)))]) / 255, \n",
    "                    np.stack([origin_img,\n",
    "                              np.array(Image.open(f'./Dataset/sample_2/{filename}_s2.png').resize((16, 16)))]) / 255, \n",
    "                    np.stack([origin_img,\n",
    "                              np.array(Image.open(f'./Dataset/sample_3/{filename}_s3.png').resize((16, 16)))]) / 255,\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames) * 4\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx % len(self.filenames)]\n",
    "        image = self.images[idx % len(self.filenames)][idx // len(self.filenames)]\n",
    "        return (filename, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2, out_channels=4, kernel_size=3, padding=1), # 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 8x8\n",
    "            nn.Conv2d(in_channels=4, out_channels=4, kernel_size=3, padding=1), # 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 4x4\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=4, out_channels=4, kernel_size=2, stride=2), # 8x8\n",
    "            nn.ConvTranspose2d(in_channels=4, out_channels=2, kernel_size=2, stride=2), # 16x16\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        r'''\n",
    "        x.size() == [bs, 2, 16, 16]\n",
    "        '''\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, h):\n",
    "        return self.decoder(h)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encode(x)\n",
    "        decoded = self.decode(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset()\n",
    "train_loader = dataloader.DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ae = CNNAutoencoder().cuda()\n",
    "model_opt = torch.optim.Adam(model_ae.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "vis = visdom.Visdom(env='Cnn autoencoder train (1)')\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ae.load_state_dict(torch.load('./cnn_ae_model.pth'))\n",
    "model_opt.load_state_dict(torch.load('./cnn_ae_model_opt.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for epoch in range(300):\n",
    "    for batch in train_loader:\n",
    "        _, images = batch\n",
    "        images = images.float().cuda()\n",
    "        model_opt.zero_grad()\n",
    "        prediction = model_ae(images)\n",
    "        loss = nn.functional.binary_cross_entropy_with_logits(prediction, images, reduction='sum')\n",
    "        loss.backward()\n",
    "        model_opt.step()\n",
    "\n",
    "        losses.append(loss.cpu().detach().item() / images.size(0))\n",
    "\n",
    "        if step % 20 == 0:\n",
    "            vis.line(X=[step], Y=[mean(losses)], update='append', name='total loss', win='losses')\n",
    "            losses = []\n",
    "\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ae.state_dict(), './cnn_ae_model.pth')\n",
    "torch.save(model_opt.state_dict(), './cnn_ae_model_opt.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_idx = {fn: i for i, fn in enumerate(ds.filenames)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Sample 1</th>\n",
       "      <th>Sample 2</th>\n",
       "      <th>Sample 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000072_000.png</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000150_002.png</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000181_061.png</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000211_019.png</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000211_041.png</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Case  Sample 1  Sample 2  Sample 3\n",
       "0  00000072_000.png         1         5         1\n",
       "1  00000150_002.png         5         5         3\n",
       "2  00000181_061.png         4         4         3\n",
       "3  00000211_019.png         4         4         2\n",
       "4  00000211_041.png         3         5         2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marks = pd.read_csv('./Dataset/OpenPart.csv')\n",
    "marks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = dict()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, row in marks.iterrows():\n",
    "        fname = row['Case'].split('.')[0]\n",
    "\n",
    "        _, pred_exp = ds[rev_idx[fname]]\n",
    "        _, pred_1 = ds[rev_idx[fname] + len(ds.filenames)]\n",
    "        _, pred_2 = ds[rev_idx[fname] + 2 * len(ds.filenames)]\n",
    "        _, pred_3 = ds[rev_idx[fname] + 3 * len(ds.filenames)]\n",
    "\n",
    "        feature_exp = model_ae.encode(torch.tensor(pred_exp).unsqueeze(0).float().cuda()).cpu().view(1, -1).numpy()\n",
    "        feature_1 = model_ae.encode(torch.tensor(pred_1).unsqueeze(0).float().cuda()).cpu().view(1, -1).numpy()\n",
    "        feature_2 = model_ae.encode(torch.tensor(pred_2).unsqueeze(0).float().cuda()).cpu().view(1, -1).numpy()\n",
    "        feature_3 = model_ae.encode(torch.tensor(pred_3).unsqueeze(0).float().cuda()).cpu().view(1, -1).numpy()\n",
    "        \n",
    "        vectors[fname] = [feature_exp, feature_1, feature_2, feature_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./cnn_vectors.pickle', 'wb') as vec_file:\n",
    "    pickle.dump(vectors, vec_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
